"""
This file contains some extra distributions that are not available in PyTorch.
The distributions include a Gaussian mixture model, a marginal distribution on
the 2D torus, and a 2D Gaussian mixture distribution. Furthermore, the file
contains a Gaussian likelihood distribution that maps all input parameters to
their absolute value, and a Gaussian likelihood distribution that only adds noise.
The `__main__` portion of the file contains some example usage and tests.
"""

import numpy as np
import torch
import torch.distributions as dist

class ClippedUniform(dist.Uniform):
    """ A uniform distribution with a lower and upper bound, which returns
        -inf for values outside the bounds. """
    def __init__(self, low: float, high: float):
        super().__init__(low, high)

    def sample(self, sample_shape=torch.Size()) -> torch.Tensor:
        return super().sample(sample_shape)

    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
        # Filter out values outside the bounds
        mask = (value >= self.low) & (value <= self.high)
        log_prob = torch.zeros_like(value)
        log_prob[mask] = super().log_prob(value[mask])
        log_prob[~mask] = -np.inf
        return log_prob


class MixtureMarginal(dist.Distribution):
    """ A Gaussian mixture model with K components.
        pi: a K-dimensional vector of mixing proportions
        mu: a K-dimensional vector of means
        sigma: a K-dimensional vector of standard deviations
       
        See below for an example usage and a plot."""
    def __init__(self, pi: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor):
        super().__init__(validate_args=False)
        self.pi = pi
        self.mu = mu
        self.sigma = sigma

        assert pi.shape == mu.shape == sigma.shape

    @staticmethod
    def gaussian_pdf(values: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor) -> torch.Tensor:
        pdf = torch.exp(-0.5 * (values - mu) ** 2 / sigma ** 2) / torch.sqrt(2 * np.pi * sigma ** 2)
        return pdf
    
    def sample(self, sample_shape=torch.Size()) -> torch.Tensor:
        if sample_shape == torch.Size():
            sample_shape = torch.Size([1])
        elif isinstance(sample_shape, int):
            sample_shape = torch.Size([sample_shape])

        cluster = dist.Categorical(self.pi).sample(sample_shape)
        x_samples = dist.Normal(self.mu[cluster], self.sigma[cluster]).sample()
        return x_samples

    def pdf(self, value: torch.Tensor) -> torch.Tensor:
        pdf = torch.zeros_like(value)
        for i, (mu, sigma) in enumerate(zip(self.mu, self.sigma)):
            pdf += self.gaussian_pdf(value, mu, sigma) * self.pi[i]
        return pdf

    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
        log_prob = torch.log(self.pdf(value))
        return log_prob.squeeze()

    
class TorusMarginal(dist.Distribution):
    """ A marginal distribution on the 2D torus.

        This distribution generates points on a 2D torus centered at `center` with
        radius `radius`, and with Gaussian noise of standard deviation `sigma`.
        The distribution is defined by the `sample` and `log_prob` methods.

        Args:
            center (torch.Tensor): The center of the torus, as a 2D tensor.
            radius (float): The radius of the torus.
            sigma (float): The standard deviation of the Gaussian noise.

        A marginal distribution on the torus. See below for an example usage and a plot. """
    def __init__(self, center: torch.Tensor, radius: float, sigma: float):
        super().__init__(validate_args=False)
        self.radius = radius
        self.center = center
        self.sigma = sigma

    def sample(self, sample_shape=torch.Size()) -> torch.Tensor:
        squeeze = False
        if sample_shape == torch.Size():
            sample_shape = torch.Size([1])
            squeeze = True
        elif isinstance(sample_shape, int):
            sample_shape = torch.Size([sample_shape])
        
        theta = dist.Uniform(0, 2 * np.pi).sample(sample_shape)
        mu = torch.stack([torch.cos(theta), torch.sin(theta)]).transpose(0, 1) * self.radius
        mu += self.center

        sigma = torch.eye(2) * self.sigma
        p_d_theta = dist.MultivariateNormal(loc=mu, covariance_matrix=sigma)

        if squeeze:
            return p_d_theta.sample().squeeze(dim=0)
        return p_d_theta.sample()

    def log_prob(self, theta: torch.Tensor) -> torch.Tensor:
        """ Approximate log probability of a point generated by the torus."""
        # First center the data
        theta = theta - self.center

        # Sample a set of centers on the circle (uniformly)
        mu_resolution = 100
        t = torch.linspace(0, 2 * np.pi, mu_resolution + 1, device=theta.device)
        t = t[:-1]
        mu_1 = torch.cos(t) * self.radius
        mu_2 = torch.sin(t) * self.radius
        mu = torch.stack([mu_1, mu_2]).transpose(0, 1)

        # Compute the distance to the sampled centers
        distances = torch.norm(theta.unsqueeze(1) - mu, dim=2)

        # Compute the log probability
        log_prob = dist.Normal(loc=0, scale=self.sigma).log_prob(distances)

        # Return the average log probability of the sampled centers
        return torch.logsumexp(log_prob, dim=1) - np.log(mu_resolution)


class MixtureMarginal2D(dist.Distribution):
    """
    MixtureMarginal2D class represents a 2D mixture marginal distribution.

    Args:
        locs (torch.Tensor): The locations of the mixture components.
        scales (torch.Tensor): The scales of the mixture components.

    Attributes:
        locs (torch.Tensor): The locations of the mixture components.
        sigma (torch.Tensor): The scales of the mixture components.

    """

    def __init__(self, locs: torch.Tensor, scales: torch.Tensor):
        super().__init__(validate_args=False)
        self.locs = locs
        self.sigma = scales

        assert locs.shape[0] == scales.shape[0] == 2

    def sample(self, sample_shape=torch.Size()) -> torch.Tensor:
        """
        Generates samples from the distribution.

        Args:
            sample_shape (torch.Size or int, optional): The shape of the samples to be generated.

        Returns:
            torch.Tensor: The generated samples.

        """
        squeeze = False
        if sample_shape == torch.Size():
            sample_shape = torch.Size([1])
            squeeze = True
        elif isinstance(sample_shape, int):
            sample_shape = torch.Size([sample_shape])

        mu = self.locs[dist.Categorical(torch.ones(len(self.locs))).sample(sample_shape)]
        sigma = torch.eye(2) * self.sigma
        p_d_theta = dist.MultivariateNormal(loc=mu, covariance_matrix=sigma)

        if squeeze:
            return p_d_theta.sample().squeeze(dim=0)
        return p_d_theta.sample()

    def log_prob(self, theta: torch.Tensor) -> torch.Tensor:
        """
        Computes the log probability of the given input.

        Args:
            theta (torch.Tensor): The input for which to compute the log probability.

        Returns:
            torch.Tensor: The log probability of the input.

        """
        p_theta_k = [dist.MultivariateNormal(loc=self.locs[k], covariance_matrix=torch.eye(2) * self.sigma) for k in range(len(self.locs))]
        prob_gaus = torch.stack([p_theta_k[k].log_prob(theta) for k in range(len(self.locs))]).exp()
        prob_pi = torch.ones(len(self.locs)) / len(self.locs)

        log_prob = (prob_pi.unsqueeze(-1) * prob_gaus).sum(dim=0).log()

        return log_prob


class GaussianAbsoluteLikelihood:
    """
    Represents a Gaussian absolute likelihood distribution.

    Args:
        event_shape (torch.Size): The shape of the event.
        sigma (float, optional): The standard deviation of the distribution. Defaults to 0.1.
    """

    def __init__(self, event_shape: torch.Size, sigma: float = 0.1):
        super().__init__()
        self.event_shape = event_shape
        self.sigma = torch.eye(self.event_shape[0]) * sigma

    def likelihood(self, theta: torch.Tensor) -> dist.Distribution:
        """
        Computes the likelihood distribution given the input theta.

        Args:
            theta (torch.Tensor): The input tensor.

        Returns:
            dist.Distribution: The likelihood distribution.
        """
        if type(theta) == float:
            theta = torch.tensor([theta])
        cov = (torch.eye(theta.shape[-1]) * self.sigma).to(theta.device)
        p_d_theta = dist.MultivariateNormal(loc=theta.abs(), covariance_matrix=cov)
        return p_d_theta

    def sample(self, theta):
        raise NotImplementedError("Should not be called.")
        self.sigma = self.sigma.to(theta.device)
        p_d_theta = dist.MultivariateNormal(loc=theta.abs(), covariance_matrix=self.sigma)
        return p_d_theta

    def log_prob(self, theta, d):
        raise NotImplementedError("Should not be called.")
        self.sigma = self.sigma.to(theta.device)
        p_d_theta = dist.MultivariateNormal(loc=theta.abs(), covariance_matrix=self.sigma)
        return p_d_theta.log_prob(d)

class GaussianNoiseLikelihood:
    """
    Represents a Gaussian noise likelihood distribution.

    Args:
        event_shape (torch.Size): The shape of the event.
        sigma (float, optional): The standard deviation of the Gaussian noise. Defaults to 0.1.
    """

    def __init__(self, event_shape: torch.Size, sigma: float = 0.1):
        super().__init__()
        self.event_shape = event_shape
        self.sigma = sigma

    def likelihood(self, theta: torch.Tensor | float) -> dist.Distribution:
        """
        Computes the likelihood distribution.

        Args:
            theta (torch.Tensor | float): The input tensor or float.

        Returns:
            dist.Distribution: The likelihood distribution.
        """
        if type(theta) == float:
            theta = torch.tensor([theta])
        cov = (torch.eye(theta.shape[-1]) * self.sigma).to(theta.device)
        p_d_theta = dist.MultivariateNormal(loc=theta, covariance_matrix=cov)
        return p_d_theta

    def sample(self, theta):
        raise NotImplementedError("Should not be called.")
        self.sigma = self.sigma.to(theta.device)
        p_d_theta = dist.MultivariateNormal(loc=theta, covariance_matrix=self.sigma)
        return p_d_theta

    def log_prob(self, theta, d):
        raise NotImplementedError("Should not be called.")
        self.sigma = self.sigma.to(theta.device)
        p_d_theta = dist.MultivariateNormal(loc=theta, covariance_matrix=self.sigma)
        return p_d_theta.log_prob(d)

def rejection_sampling_uniform(fn_theta: callable,
                               interval: tuple | list,
                               n_samples: int,
                               shape: int = 1,
                               log_prob: bool = False,
                               tqdm_output: bool = False
                               ) -> torch.Tensor:
    """Implements rejection sampling for a uniform proposal distribution. The
       input argument `fn_theta` is assumed to return probabilities."""
    samples = torch.tensor([])
    min, max = interval

    while len(samples) < n_samples:
        # proposal distribution is uniform over range
        q = torch.rand((n_samples, shape)) * (max - min) + min

        # sample from a uniform distribution over [0, 1]
        u = torch.rand((n_samples))

        # accept if u < p_tilde / q_tilde
        p_tilde = torch.log(fn_theta(q)) if not log_prob else fn_theta(q)
        q_tilde = dist.Uniform(min, max).log_prob(q)

        if shape > 1:
            q_tilde = q_tilde.sum(dim=-1)

        accept = (u.log() < p_tilde - q_tilde)  # .all(dim=1)
        samples = torch.cat((samples, q[accept]))

    return samples[:n_samples]

if __name__ == "__main__":
    import matplotlib.pyplot as plt
    import seaborn as sns
    print("Example usage / tests")
    PLOTTING = True

    # Test the mixture model
    pi = torch.tensor([0.5, 0.5])
    mu = torch.tensor([-1.0, 1.0])
    sigma = torch.tensor([0.1, 0.1])

    mixture = MixtureMarginal(pi, mu, sigma)
    point = mixture.sample()
    assert point.shape == torch.Size([1])

    N = 10000
    x = mixture.sample((N,))
    assert x.shape == torch.Size([N])

    log_prob = mixture.log_prob(x)
    assert log_prob.shape == torch.Size([N])

    # Plot the samples
    if PLOTTING:
        fig, ax = plt.subplots()
        ax.hist(x, bins=100)
        plt.show()

    # Test the torus marginal
    center = torch.tensor([0.0, 0.0])
    radius = 2.0
    sigma = 0.1

    marginal = TorusMarginal(center, radius, sigma)
    point = marginal.sample()
    assert point.shape == (2,)

    N = 10000
    theta = marginal.sample((N,))
    assert theta.shape == (N, 2)
    log_prob = marginal.log_prob(theta)
    assert log_prob.shape == (N,)

    # Plot the samples
    if PLOTTING:
        fig, ax = plt.subplots()
        ax.scatter(theta[:, 0], theta[:, 1], s=1)
        plt.show()

    # Test the 2D mixture marginal
    locs = torch.tensor([[-1.0, 0.0], [1.0, 0.0]])
    sigma = torch.tensor([0.1, 0.1])

    marginal = MixtureMarginal2D(locs, sigma)
    point = marginal.sample()
    assert point.shape == (2,)

    N = 10000
    theta = marginal.sample((N,))
    assert theta.shape == (N, 2)
    log_prob = marginal.log_prob(theta)
    assert log_prob.shape == (N,)

    # Plot the samples
    if PLOTTING:
        fig, ax = plt.subplots()
        ax.scatter(theta[:, 0], theta[:, 1], s=1)
        plt.show()
